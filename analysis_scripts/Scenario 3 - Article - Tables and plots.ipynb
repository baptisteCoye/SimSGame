{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 3 - tables and plots for the paper\n",
    "\n",
    "This document contains an advanced analysis of the results obtained with our scheduling simulator.\n",
    "It resumes the tables and plots that we are using on our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General information**\n",
    "\n",
    "These simulation results were generated from 4 to 20 resources. Each configuration was run with 50 different RNG seeds (1 up to 50).\n",
    "\n",
    "Each simulation is composed of 200 frames. Lag starts at zero and increases by 0.01 with each frame up to a lag equal to 100% in frame 101. After that, the lag starts to decrease in the same rhythm down to 0.01 in frame 200.\n",
    "\n",
    "\n",
    "**Algorithms abbreviation in presentation order:**\n",
    "\n",
    "FIFO serves as the baseline for comparisons.\n",
    "\n",
    "1. **FIFO:** First In First Out.\n",
    "2. **LPT:** Longest Processing Time First.\n",
    "3. **SPT:** Shortest Processing Time First.\n",
    "4. **SLPT:** LPT at a subtask level.\n",
    "5. **SSPT:** SPT at a subtask level.\n",
    "6. **HRRN:** Highest Response Ratio Next. \n",
    "7. **WT:** Longest Waiting Time First.\n",
    "8. **HLF:** Hu's Level First with unitary processing time of each task.\n",
    "9. **HLFET:** HLF with estimated times.\n",
    "10. **CG:** Coffman-Graham's Algorithm.\n",
    "11. **DCP:** Dynamic Critical Path Priority.\n",
    "\n",
    "**Metrics:**\n",
    "* SF: slowest frame (maximum frame execution time)\n",
    "* DF: number of delayed frames (with 16.667 ms as the due date)\n",
    "* CS: cummulative slowdown (with 16.667 ms as the due date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table III\n",
    "**Average values for the different metrics and algorithms with 12 resources and comparison to the previous results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "seeds = [str(i) for i in range(1,51)]\n",
    "algorithms = [\"FIFO\", \"LPT\", \"SPT\", \"SLRT\", \"SSRT\", \"HRRN\", \"WT\", \"HLF\", \"Hu\", \"Coffman\", \"Priority\"]\n",
    "finalnames = [\"FIFO\", \"LPT\", \"SPT\", \"SLPT\", \"SSPT\", \"HRRN\", \"WT\", \"HLF\", \"HLFET\", \"CG\", \"DCP\"]\n",
    "resources = [i for i in range (4,21)]\n",
    "\n",
    "directory = \"../Result_3/\"\n",
    "directory_sorted = \"../Result_2/\"\n",
    "directory_base = \"../Result_1/\"\n",
    "directory_cp = \"../Result_CP_3/\"\n",
    "directory_cp_base = \"../Result_CP_1/\"\n",
    "\n",
    "colors = sns.color_palette('hls', n_colors=13)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the path and file name\n",
    "def file_name_and_path(directory, algorithm, resources, seed, extra):\n",
    "    filename = directory + algorithm + \"/\" + resources + \"/200/TXT/\" + \\\n",
    "               algorithm + extra + seed + \"_200_\" + resources + \".txt\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table(algorithms, names, seeds, directory, resources, extra):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s, extra)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = generate_table(algorithms, finalnames, seeds, directory, '12', '_divided_Random_')\n",
    "table_base = generate_table(algorithms, finalnames, seeds, directory_base, '12', '_NonSorted_Random_')\n",
    "table.to_csv('../table_4___average_metrics_12_resources.csv', header=False, sep='&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sorted = table.transpose().set_index('Algorithms')\n",
    "t_base = table_base.transpose().set_index('Algorithms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dif = (-(t_base - t_sorted)/ t_base).multiply(100).transpose()\n",
    "t_dif.to_csv('../table_4___diff_metrics_12_resources.csv', sep='&')\n",
    "t_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '4', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '8', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '12', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '16', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '20', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "1. Comparisons between FIFO and LPT, WT, CG, and DCP\n",
    "2. Comparisons between WT, CG, and DCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['SF', 'DF', 'CS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources\n",
    "def gather_all_data(algorithms, names, seeds, directory, resources, extra):\n",
    "    k = []\n",
    "    for i in range(len(algorithms)):\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algorithms[i], str(resources), s, extra)\n",
    "            df = pd.read_csv(filename, sep=' ', header=None)\n",
    "            k1 = []\n",
    "            k1.append(names[i])\n",
    "            k1.append(s)\n",
    "            k1.append(np.max(df[1])/1000)\n",
    "            k1.append(len([x for x in df[1] if x > 16667]))\n",
    "            k1.append(sum([x - 16667 for x in df[1] if x > 16667])/1000)\n",
    "            k.append(k1)\n",
    "\n",
    "    df = pd.DataFrame(k)\n",
    "    df.columns = ['Algorithm', 'Seed', 'SF', 'DF', 'CS']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gather_all_data(algorithms, finalnames, seeds, directory, 12, '_divided_Random_')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS tests - applied for each algorithm and each metric\n",
    "algos = ['FIFO', 'LPT', 'WT', 'CG', 'DCP']\n",
    "sfs = {}\n",
    "dfs = {}\n",
    "css = {}\n",
    "\n",
    "np.random.seed(2021) # setting RNG seed for analysis\n",
    "\n",
    "# preparing the lists of values\n",
    "for algo in algos:\n",
    "    sfs[algo] = list(data.loc[(data['Algorithm'] == algo)].SF)\n",
    "    dfs[algo] = list(data.loc[(data['Algorithm'] == algo)].DF)\n",
    "    css[algo] = list(data.loc[(data['Algorithm'] == algo)].CS)\n",
    "\n",
    "# running the KS test\n",
    "for algo in algos:\n",
    "    print(f'KS test for scheduler {algo} and the Slowest Frame metric')\n",
    "    results = sfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Delayed Frames metric')\n",
    "    results = dfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Cummulative Slowdown metric')\n",
    "    results = css[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so we cannot reject the null hypothesis that these results come from normal distributions.\n",
    "We will then apply an F-test to see if pairs of results have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-test function\n",
    "def f_test(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
    "    dfn = x.size - 1 #define degrees of freedom numerator \n",
    "    dfd = y.size - 1 #define degrees of freedom denominator \n",
    "    \n",
    "    p = 1 - stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = f_test(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = f_test(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Cummulative Slowdown metric')\n",
    "        F, p_value = f_test(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}\\n')\n",
    "    print('-----------')    \n",
    "\n",
    "# Comparing the variances for FIFO and other schedulers\n",
    "run_f_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# Comparing variances for WT and other schedulers\n",
    "run_f_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# Comparing variances for CG and other schedulers\n",
    "run_f_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so it seems our variances are all equal.\n",
    "We can move to standard t-tests in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = stats.ttest_rel(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = stats.ttest_rel(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Cummulative Slowdown metric')\n",
    "        F, p_value = stats.ttest_rel(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}\\n')\n",
    "    print('-----------')    \n",
    "\n",
    "# T-test for FIFO and other schedulers\n",
    "run_t_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# T-test for WT and other schedulers\n",
    "run_t_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# T-test for CG and other schedulers\n",
    "run_t_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumed view of these results:\n",
    "\n",
    "1. WT, CG, and DCP show better results than FIFO (p-values < 0.05)\n",
    "2. WT performed better than DCP (p-values < 0.05)\n",
    "3. WT performed the same as CG for metrics SF and DF (p-values = 0.31 and 0.06, resp.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information from the critical path for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table_cp(algorithms, names, seeds, directory, resources, extra):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s, extra)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = generate_table_cp(['Infinity'], ['Critical Path'], seeds, directory_cp, '1000', '_divided_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_base_data = generate_table_cp(['Infinity'], ['Critical Path'], seeds, directory_cp_base, '1000', '_NonSorted_Random_')\n",
    "cp_base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-((cp_base_data[0][1] - cp_data[0][1])/ cp_base_data[0][1]) * 100)\n",
    "print(-((cp_base_data[0][2] - cp_data[0][2])/ cp_base_data[0][2]) * 100)\n",
    "print(-((cp_base_data[0][3] - cp_data[0][3])/ cp_base_data[0][3]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 7\n",
    "**Lineplots with the duration of the frames for an algorithm and the critical path in different configurations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe for one number of resources and algorithm\n",
    "def gather_line_data(algorithm, seeds, directory, resource, extra):\n",
    "    data = []\n",
    "    for s in seeds:\n",
    "        filename = file_name_and_path(directory, algorithm, resource, s, extra)\n",
    "        df = pd.read_csv(filename, sep=' ', header=None)\n",
    "        data += [df]\n",
    "    result = pd.concat(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(5, 27.500)\n",
    "\n",
    "last_palette = sns.dark_palette(colors[9], 12)\n",
    "gray_palette = sns.dark_palette('gray', 12)\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory_base, '12', '_NonSorted_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 12 res. (orig.)', color=last_palette[-1])\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory_sorted, '12', '_sorted_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 12 res. (sorted)', color=last_palette[-3])\n",
    "\n",
    "result = gather_line_data('Infinity', seeds, directory_cp_base, '1000', '_NonSorted_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='Critical Path (orig.)', color=gray_palette[-1])\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory, '12', '_divided_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 12 res. (+ par.)', color=last_palette[-5])\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory, '20', '_divided_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 20 res. (+ par.)', color=last_palette[-7])\n",
    "\n",
    "result = gather_line_data('Infinity', seeds, directory_cp, '1000', '_divided_Random_')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='Critical Path (+ par.)', color=gray_palette[-3])\n",
    "\n",
    "sns.lineplot(x=list(range(1,201)), y=200*[16.667], ci=None, label='16.667 ms due date', color='red')\n",
    "\n",
    "plt.xlabel('Frames', fontsize=fontsize)\n",
    "plt.ylabel('Average Frame Duration (ms)', fontsize=fontsize)\n",
    "plt.xticks(range(0,201,25))\n",
    "plt.legend(bbox_to_anchor=(1.02, 0.78), loc='upper left', borderaxespad=0, fontsize=(fontsize-1))\n",
    "\n",
    "plt.savefig(\"../results-7-frame-duration.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix - input files used to generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_scenario_3.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_CP_scenario_3.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
