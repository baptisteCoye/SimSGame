{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2 - tables and plots for the paper\n",
    "\n",
    "This document contains an advanced analysis of the results obtained with our scheduling simulator.\n",
    "It resumes the tables and plots that we are using on our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General information**\n",
    "\n",
    "These simulation results were generated from 4 to 20 resources. Each configuration was run with 50 different RNG seeds (1 up to 50).\n",
    "\n",
    "Each simulation is composed of 200 frames. Lag starts at zero and increases by 0.01 with each frame up to a lag equal to 100% in frame 101. After that, the lag starts to decrease in the same rhythm down to 0.01 in frame 200.\n",
    "\n",
    "\n",
    "**Algorithms abbreviation in presentation order:**\n",
    "\n",
    "FIFO serves as the baseline for comparisons.\n",
    "\n",
    "1. **FIFO:** First In First Out.\n",
    "2. **LPT:** Longest Processing Time First.\n",
    "3. **SPT:** Shortest Processing Time First.\n",
    "4. **SLPT:** LPT at a subtask level.\n",
    "5. **SSPT:** SPT at a subtask level.\n",
    "6. **HRRN:** Highest Response Ratio Next. \n",
    "7. **WT:** Longest Waiting Time First.\n",
    "8. **HLF:** Hu's Level First with unitary processing time of each task.\n",
    "9. **HLFET:** HLF with estimated times.\n",
    "10. **CG:** Coffman-Graham's Algorithm.\n",
    "11. **DCP:** Dynamic Critical Path Priority.\n",
    "\n",
    "**Metrics:**\n",
    "* SF: slowest frame (maximum frame execution time)\n",
    "* DF: number of delayed frames (with 16.667 ms as the due date)\n",
    "* CS: cumulative slowdown (with 16.667 ms as the due date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table III\n",
    "**Average values for the different metrics and algorithms with 12 resources and comparison to the previous results**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "seeds = [str(i) for i in range(1,51)]\n",
    "algorithms = [\"FIFO\", \"LPT\", \"SPT\", \"SLRT\", \"SSRT\", \"HRRN\", \"WT\", \"HLF\", \"Hu\", \"Coffman\", \"Priority\"]\n",
    "finalnames = [\"FIFO\", \"LPT\", \"SPT\", \"SLPT\", \"SSPT\", \"HRRN\", \"WT\", \"HLF\", \"HLFET\", \"CG\", \"DCP\"]\n",
    "resources = [i for i in range (4,21)]\n",
    "\n",
    "directory = \"../Result_2/\"\n",
    "directory_base = \"../Result_1/\"\n",
    "directory_cp = \"../Result_CP_1/\"\n",
    "\n",
    "\n",
    "colors = sns.color_palette('hls', n_colors=13)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the path and file name\n",
    "def file_name_and_path(directory, algorithm, resources, seed, extra):\n",
    "    filename = directory + algorithm + \"/\" + resources + \"/200/TXT/\" + \\\n",
    "               algorithm + extra + seed + \"_200_\" + resources + \".txt\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table(algorithms, names, seeds, directory, resources, extra):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s, extra)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = generate_table(algorithms, finalnames, seeds, directory, '12', '_sorted_Random_')\n",
    "table_base = generate_table(algorithms, finalnames, seeds, directory_base, '12', '_NonSorted_Random_')\n",
    "table.to_csv('../table_3___average_metrics_12_resources.csv', header=False, sep='&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sorted = table.transpose().set_index('Algorithms')\n",
    "t_base = table_base.transpose().set_index('Algorithms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dif = (-(t_base - t_sorted)/ t_base).multiply(100).transpose()\n",
    "t_dif.to_csv('../table_3___diff_metrics_12_resources.csv', sep='&')\n",
    "t_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '4', '_sorted_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '8', '_sorted_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '12', '_sorted_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '16', '_sorted_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '20', '_sorted_Random_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "1. Comparisons between FIFO and LPT, WT, CG, and DCP\n",
    "2. Comparisons between WT, CG, and DCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['SF', 'DF', 'CS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources\n",
    "def gather_all_data(algorithms, names, seeds, directory, resources, extra):\n",
    "    k = []\n",
    "    for i in range(len(algorithms)):\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algorithms[i], str(resources), s, extra)\n",
    "            df = pd.read_csv(filename, sep=' ', header=None)\n",
    "            k1 = []\n",
    "            k1.append(names[i])\n",
    "            k1.append(s)\n",
    "            k1.append(np.max(df[1])/1000)\n",
    "            k1.append(len([x for x in df[1] if x > 16667]))\n",
    "            k1.append(sum([x - 16667 for x in df[1] if x > 16667])/1000)\n",
    "            k.append(k1)\n",
    "\n",
    "    df = pd.DataFrame(k)\n",
    "    df.columns = ['Algorithm', 'Seed', 'SF', 'DF', 'CS']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gather_all_data(algorithms, finalnames, seeds, directory, 12, '_sorted_Random_')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS tests - applied for each algorithm and each metric\n",
    "algos = ['FIFO', 'LPT', 'WT', 'CG', 'DCP']\n",
    "sfs = {}\n",
    "dfs = {}\n",
    "css = {}\n",
    "\n",
    "np.random.seed(2021) # setting RNG seed for analysis\n",
    "\n",
    "# preparing the lists of values\n",
    "for algo in algos:\n",
    "    sfs[algo] = list(data.loc[(data['Algorithm'] == algo)].SF)\n",
    "    dfs[algo] = list(data.loc[(data['Algorithm'] == algo)].DF)\n",
    "    css[algo] = list(data.loc[(data['Algorithm'] == algo)].CS)\n",
    "\n",
    "# running the KS test\n",
    "for algo in algos:\n",
    "    print(f'KS test for scheduler {algo} and the Slowest Frame metric')\n",
    "    results = sfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Delayed Frames metric')\n",
    "    results = dfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Cumulative Slowdown metric')\n",
    "    results = css[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so we cannot reject the null hypothesis that these results come from normal distributions.\n",
    "We will then apply an F-test to see if pairs of results have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-test function\n",
    "def f_test(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
    "    dfn = x.size - 1 #define degrees of freedom numerator \n",
    "    dfd = y.size - 1 #define degrees of freedom denominator \n",
    "    \n",
    "    p = 1 - stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = f_test(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = f_test(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Cumulative Slowdown metric')\n",
    "        F, p_value = f_test(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}\\n')\n",
    "    print('-----------')    \n",
    "\n",
    "# Comparing the variances for FIFO and other schedulers\n",
    "run_f_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# Comparing variances for WT and other schedulers\n",
    "run_f_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# Comparing variances for CG and other schedulers\n",
    "run_f_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so it seems our variances are all equal.\n",
    "We can move to standard t-tests in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = stats.ttest_rel(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = stats.ttest_rel(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Cumulative Slowdown metric')\n",
    "        F, p_value = stats.ttest_rel(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}\\n')\n",
    "    print('-----------')    \n",
    "\n",
    "# T-test for FIFO and other schedulers\n",
    "run_t_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# T-test for WT and other schedulers\n",
    "run_t_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# T-test for CG and other schedulers\n",
    "run_t_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumed view of these results:\n",
    "\n",
    "1. Besides LPT and the DF metric, all compared schedulers provide results that are different than FIFO (p-values < 0.05).\n",
    "2. WT is only different to CG and DCP in the DF and CS metrics. \n",
    "3. CG and DCP cannot be differentiated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information from the critical path for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table_cp(algorithms, names, seeds, directory, resources, extra):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s, extra)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = generate_table_cp(['Infinity'], ['Critical Path'], seeds, directory_cp, '1000', '_NonSorted_Random_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6\n",
    "**Lineplots showing the evolution of metrics when the number of resources changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources\n",
    "def gather_all_data(algorithms, names, seeds, directory, resources, extra):\n",
    "    k = []\n",
    "    for i in range(len(algorithms)):\n",
    "        for r in resources:\n",
    "            for s in seeds:\n",
    "                filename = file_name_and_path(directory, algorithms[i], str(r), s, extra)\n",
    "                df = pd.read_csv(filename, sep=' ', header=None)\n",
    "                k1 = []\n",
    "                k1.append(names[i])\n",
    "                k1.append(r)\n",
    "                k1.append(s)\n",
    "                k1.append(np.max(df[1])/1000)\n",
    "                k1.append(len([x for x in df[1] if x > 16667]))\n",
    "                k1.append(sum([x - 16667 for x in df[1] if x > 16667])/1000)\n",
    "                k.append(k1)\n",
    "    df = pd.DataFrame(k)\n",
    "    df.columns = ['Algorithm', 'Resources', 'Seed', 'SF', 'DF', 'CS']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gather_all_data(algorithms, finalnames, seeds, directory, resources, '_sorted_Random_')\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the boxplots for a given number of resources and metric\n",
    "def plot_lineplots(data, algorithm, metrics):\n",
    "    color = {metrics[0]:'blue', metrics[1]:'orange', metrics[2]:'green'}\n",
    "    for metric in metrics:\n",
    "        fig,ax = plt.subplots(1,1, figsize=(15,12))\n",
    "        sns.lineplot(x='Resources', y=metric, ci='sd', data=data[algorithm],\n",
    "                     marker='o', color=color[metric])\n",
    "        ax.set(title=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = all_data.loc[all_data['Algorithm'].isin(['FIFO', 'LPT', 'WT', 'CG', 'DCP'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(4, 20)\n",
    "plt.ylim(27.5, 47.5)\n",
    "\n",
    "sns.lineplot(x='Resources', y='SF', ci=None, data=plot_data, marker='o', hue='Algorithm',\n",
    "             palette=[colors[0], colors[1], colors[6], colors[9], colors[10]])\n",
    "sns.lineplot(x=resources, y=17*[cp_data[0][1]], ci=None, color='gray', label='Critical Path')\n",
    "plt.ylabel('Average Slowest Frame (ms)', fontsize=fontsize)\n",
    "plt.legend(fontsize=(fontsize-1))\n",
    "plt.xticks(range(4,21,2))\n",
    "\n",
    "plt.savefig(\"../results-5-resources-sf.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(4, 20)\n",
    "plt.ylim(40, 140)\n",
    "\n",
    "sns.lineplot(x='Resources', y='DF', ci=None, data=plot_data, marker='o', hue='Algorithm',\n",
    "             palette=[colors[0], colors[1], colors[6], colors[9], colors[10]])\n",
    "sns.lineplot(x=resources, y=17*[cp_data[0][2]], ci=None, color='gray', label='Critical Path')\n",
    "plt.ylabel('Average Delayed Frames (frames)', fontsize=fontsize)\n",
    "plt.legend(fontsize=(fontsize-1))\n",
    "plt.xticks(range(4,21,2))\n",
    "\n",
    "plt.savefig(\"../results-5-resources-df.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(4, 20)\n",
    "#plt.ylim(0, 650)\n",
    "\n",
    "sns.lineplot(x='Resources', y='CS', ci=None, data=plot_data, marker='o', hue='Algorithm')\n",
    "sns.lineplot(x=resources, y=17*[cp_data[0][3]], ci=None, color='gray', label='Critical Path')\n",
    "plt.ylabel('Average Cumulative Slowdown (ms)', fontsize=fontsize)\n",
    "plt.legend(fontsize=(fontsize-1))\n",
    "plt.xticks(range(4,21,2))\n",
    "\n",
    "plt.savefig(\"../results-5-resources-cs.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix - input files used to generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_scenario_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_CP_scenario_1.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
