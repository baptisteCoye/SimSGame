{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1 - tables and plots for the paper\n",
    "\n",
    "This document contains an advanced analysis of the results obtained with our scheduling simulator.\n",
    "It resumes the tables and plots that we are using on our paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General information**\n",
    "\n",
    "These simulation results were generated from 4 to 20 resources. Each configuration was run with 50 different RNG seeds (1 up to 50).\n",
    "\n",
    "Each simulation is composed of 200 frames. Lag starts at zero and increases by 0.01 with each frame up to a lag equal to 100% in frame 101. After that, the lag starts to decrease in the same rhythm down to 0.01 in frame 200.\n",
    "\n",
    "\n",
    "**Algorithms abbreviation in presentation order:**\n",
    "\n",
    "FIFO serves as the baseline for comparisons.\n",
    "\n",
    "1. **FIFO:** First In First Out.\n",
    "2. **LPT:** Longest Processing Time First.\n",
    "3. **SPT:** Shortest Processing Time First.\n",
    "4. **SLPT:** LPT at a subtask level.\n",
    "5. **SSPT:** SPT at a subtask level.\n",
    "6. **HRRN:** Highest Response Ratio Next. \n",
    "7. **WT:** Longest Waiting Time First.\n",
    "8. **HLF:** Hu's Level First with unitary processing time of each task.\n",
    "9. **HLFET:** HLF with estimated times.\n",
    "10. **CG:** Coffman-Graham's Algorithm.\n",
    "11. **DCP:** Dynamic Critical Path Priority.\n",
    "\n",
    "**Metrics:**\n",
    "* SF: slowest frame (maximum frame execution time)\n",
    "* DF: number of delayed frames (with 16.667 ms as the due date)\n",
    "* CS: cumulative slowdown (with 16.667 ms as the due date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table I\n",
    "**Average values for the different metrics and algorithms with 12 resources**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "seeds = [str(i) for i in range(1,51)]\n",
    "algorithms = [\"FIFO\", \"LPT\", \"SPT\", \"SLRT\", \"SSRT\", \"HRRN\", \"WT\", \"HLF\", \"Hu\", \"Coffman\", \"Priority\"]\n",
    "finalnames = [\"FIFO\", \"LPT\", \"SPT\", \"SLPT\", \"SSPT\", \"HRRN\", \"WT\", \"HLF\", \"HLFET\", \"CG\", \"DCP\"]\n",
    "resources = [i for i in range (4,21)]\n",
    "\n",
    "directory = \"../Result_1/\"\n",
    "directory_cp = \"../Result_CP_1/\"\n",
    "\n",
    "colors = sns.color_palette('hls', n_colors=13)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the path and file name\n",
    "def file_name_and_path(directory, algorithm, resources, seed):\n",
    "    filename = directory + algorithm + \"/\" + resources + \"/200/TXT/\" + \\\n",
    "               algorithm + \"_NonSorted_Random_\" + seed + \"_200_\" + resources + \".txt\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table(algorithms, names, seeds, directory, resources):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = generate_table(algorithms, finalnames, seeds, directory, '12')\n",
    "table.to_csv('../table_2___average_metrics_12_resources.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_table(algorithms, finalnames, seeds, directory, '20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1\n",
    "**Boxplots of the different metrics for all algorithms and 12 resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['SF', 'DF', 'CS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources\n",
    "def gather_all_data(algorithms, names, seeds, directory, resources):\n",
    "    k = []\n",
    "    for i in range(len(algorithms)):\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algorithms[i], str(resources), s)\n",
    "            df = pd.read_csv(filename, sep=' ', header=None)\n",
    "            k1 = []\n",
    "            k1.append(names[i])\n",
    "            k1.append(s)\n",
    "            k1.append(np.max(df[1])/1000)\n",
    "            k1.append(len([x for x in df[1] if x > 16667]))\n",
    "            k1.append(sum([x - 16667 for x in df[1] if x > 16667])/1000)\n",
    "            k.append(k1)\n",
    "\n",
    "    df = pd.DataFrame(k)\n",
    "    df.columns = ['Algorithm', 'Seed', 'SF', 'DF', 'CS']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gather_all_data(algorithms, finalnames, seeds, directory, 12)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(26, 40)\n",
    "\n",
    "sns.boxplot(x=data['Algorithm'], y=data[metrics[0]], data=data, palette=colors)\n",
    "plt.ylabel('Slowest Frame (ms)', fontsize=fontsize)\n",
    "plt.xlabel('Scheduling algorithms', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-1-sf.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(59, 85)\n",
    "\n",
    "sns.boxplot(x=data['Algorithm'], y=data[metrics[1]], data=data, palette=colors)\n",
    "plt.ylabel('Delayed Frames (frames)', fontsize=fontsize)\n",
    "plt.xlabel('Scheduling algorithms', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-1-df.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(270, 450)\n",
    "\n",
    "sns.boxplot(x=data['Algorithm'], y=data[metrics[2]], data=data, palette=colors)\n",
    "plt.ylabel('Cumulative Slowdown (ms)', fontsize=fontsize)\n",
    "plt.xlabel('Scheduling algorithms', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-1-cs.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "1. Comparisons between FIFO and LPT, WT, CG, and DCP\n",
    "2. Comparisons between WT, CG, and DCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS tests - applied for each algorithm and each metric\n",
    "algos = ['FIFO', 'LPT', 'WT', 'CG', 'DCP']\n",
    "sfs = {}\n",
    "dfs = {}\n",
    "css = {}\n",
    "\n",
    "np.random.seed(2021) # setting RNG seed for analysis\n",
    "\n",
    "# preparing the lists of values\n",
    "for algo in algos:\n",
    "    sfs[algo] = list(data.loc[(data['Algorithm'] == algo)].SF)\n",
    "    dfs[algo] = list(data.loc[(data['Algorithm'] == algo)].DF)\n",
    "    css[algo] = list(data.loc[(data['Algorithm'] == algo)].CS)\n",
    "\n",
    "# running the KS test\n",
    "for algo in algos:\n",
    "    print(f'KS test for scheduler {algo} and the Slowest Frame metric')\n",
    "    results = sfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Delayed Frames metric')\n",
    "    results = dfs[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print(f'\\nKS test for scheduler {algo} and the Cumulative Slowdown metric')\n",
    "    results = css[algo]\n",
    "    print(stats.kstest(results, 'norm', args=(np.mean(results), np.std(results))))\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so we cannot reject the null hypothesis that these results come from normal distributions.\n",
    "We will then apply an F-test to see if pairs of results have the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-test function\n",
    "def f_test(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    f = np.var(x, ddof=1)/np.var(y, ddof=1) #calculate F test statistic \n",
    "    dfn = x.size - 1 #define degrees of freedom numerator \n",
    "    dfd = y.size - 1 #define degrees of freedom denominator \n",
    "    \n",
    "    p = 1 - stats.f.cdf(f, dfn, dfd) #find p-value of F test statistic \n",
    "    return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = f_test(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = f_test(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'F-test for schedulers {standard} and {algo} and the Cumulative Slowdown metric')\n",
    "        F, p_value = f_test(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(' ')\n",
    "    print('-----------')    \n",
    "\n",
    "# Comparing the variances for FIFO and other schedulers\n",
    "run_f_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# Comparing variances for WT and other schedulers\n",
    "run_f_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# Comparing variances for CG and other schedulers\n",
    "run_f_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No p-values under 0.05, so it seems our variances are all equal.\n",
    "We can move to standard t-tests in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_t_tests(standard, algos, sfs, dfs, css):\n",
    "    for algo in algos:\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Slowest Frame metric')\n",
    "        F, p_value = stats.ttest_rel(sfs[standard], sfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Delayed Frames metric')\n",
    "        F, p_value = stats.ttest_rel(dfs[standard], dfs[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(f'T-test for schedulers {standard} and {algo} and the Cumulative Slowdown metric')\n",
    "        F, p_value = stats.ttest_rel(css[standard], css[algo])\n",
    "        print(f'p-value = {p_value}')\n",
    "        print(' ')\n",
    "    print('-----------')    \n",
    "\n",
    "# T-test for FIFO and other schedulers\n",
    "run_t_tests('FIFO', ['LPT', 'WT', 'CG', 'DCP'], sfs, dfs, css)\n",
    " \n",
    "# T-test for WT and other schedulers\n",
    "run_t_tests('WT', ['CG', 'DCP'], sfs, dfs, css)\n",
    "   \n",
    "# T-test for CG and other schedulers\n",
    "run_t_tests('CG', ['DCP'], sfs, dfs, css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumed view of these results:\n",
    "\n",
    "1. Besides LPT and the SF metric, all compared schedulers provide results that are different than FIFO (p-values < 0.05).\n",
    "2. WT is only different to CG and DCP in the CS metric. This is also the case between CG and DCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2\n",
    "**Histograms of frame duration reduction when compared to the baseline (FIFO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources and algorithm\n",
    "def gather_hist_data(algorithms, names, seeds, directory, r):\n",
    "    results = {}  # dictionary per algorithm\n",
    "    for i in range(len(algorithms)):\n",
    "        if algorithms[i] == \"FIFO\": # nothing to do for FIFO here\n",
    "            continue\n",
    "        results[names[i]] = {}  # dictionary per number of resources\n",
    "        k = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algorithms[i], str(r), s)\n",
    "            df = pd.read_csv(filename, sep=' ', header=None)\n",
    "            fifoname = file_name_and_path(directory, \"FIFO\", str(r), s)\n",
    "            df_fifo = pd.read_csv(fifoname, sep=' ', header=None)\n",
    "            k.append([x-y for x,y in zip(df_fifo[1],df[1])])\n",
    "                \n",
    "        flat_list = [item for sublist in k for item in sublist]\n",
    "        results[names[i]] = pd.DataFrame(flat_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = gather_hist_data(algorithms, finalnames, seeds, directory, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_algos = ['LPT', 'WT', 'CG', 'DCP']\n",
    "\n",
    "# Function that returns the maximum and minimum values for a given algorithm\n",
    "def max_and_min(data, algorithms):\n",
    "    maximums = []\n",
    "    minimums = []\n",
    "    for algo in algorithms:\n",
    "        maximums.append(max(data[algo][0]))\n",
    "        minimums.append(min(data[algo][0]))\n",
    "    return max(maximums), min(minimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_and_min(hist_data, hist_algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the histrograms\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(-1000, 1000)\n",
    "plt.ylim(0, 650)\n",
    "\n",
    "sns.histplot(hist_data[hist_algos[0]], legend=False,  palette=[colors[1]], binwidth=20, kde=True)\n",
    "plt.ylabel('Frequency', fontsize=fontsize)\n",
    "plt.xlabel('Frame duration reduction vs. FIFO', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-2-lpt.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(-1000, 1000)\n",
    "plt.ylim(0, 650)\n",
    "\n",
    "sns.histplot(hist_data[hist_algos[1]], legend=False, palette=[colors[6]], binwidth=20, kde=True)\n",
    "plt.ylabel('Frequency', fontsize=fontsize)\n",
    "plt.xlabel('Frame duration reduction vs. FIFO', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-2-wt.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(-1000, 1000)\n",
    "plt.ylim(0, 650)\n",
    "\n",
    "sns.histplot(hist_data[hist_algos[2]], legend=False, palette=[colors[9]], binwidth=20, kde=True)\n",
    "plt.ylabel('Frequency', fontsize=fontsize)\n",
    "plt.xlabel('Frame duration reduction vs. FIFO', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-2-cg.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(-1000, 1000)\n",
    "plt.ylim(0, 650)\n",
    "\n",
    "sns.histplot(hist_data[hist_algos[3]], legend=False, palette=[colors[10]], binwidth=25, kde=True)\n",
    "plt.ylabel('Frequency', fontsize=fontsize)\n",
    "plt.xlabel('Frame duration reduction vs. FIFO', fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"../results-2-dcp.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information from the critical path for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tables with average results\n",
    "def generate_table_cp(algorithms, names, seeds, directory, resources):\n",
    "    sf = []\n",
    "    df = []\n",
    "    cs = []\n",
    "    for algo in algorithms:\n",
    "        accum_sf = []\n",
    "        accum_df = []\n",
    "        accum_cs = []\n",
    "        for s in seeds:\n",
    "            filename = file_name_and_path(directory, algo, resources, s)\n",
    "            data = pd.read_csv(filename, sep=' ', header=None)\n",
    "            accum_sf.append(np.max(data[1]))\n",
    "            accum_df.append(len([x for x in data[1] if x > 16667]))\n",
    "            accum_cs.append(sum([x - 16667 for x in data[1] if x > 16667]))\n",
    "        sf.append(np.mean(accum_sf)/1000)\n",
    "        df.append(np.mean(accum_df))\n",
    "        cs.append(np.mean(accum_cs)/1000)\n",
    "    \n",
    "    data = {'Algorithms':names, 'SF (ms)':sf, 'DF (frames)':df, 'CS (ms)':cs}\n",
    "    return pd.DataFrame(data).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data = generate_table_cp(['Infinity'], ['Critical Path'], seeds, directory_cp, '1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3\n",
    "**Lineplots showing the evolution of metrics when the number of resources changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe per number of resources\n",
    "def gather_all_data(algorithms, names, seeds, directory, resources):\n",
    "    k = []\n",
    "    for i in range(len(algorithms)):\n",
    "        for r in resources:\n",
    "            for s in seeds:\n",
    "                filename = file_name_and_path(directory, algorithms[i], str(r), s)\n",
    "                df = pd.read_csv(filename, sep=' ', header=None)\n",
    "                k1 = []\n",
    "                k1.append(names[i])\n",
    "                k1.append(r)\n",
    "                k1.append(s)\n",
    "                k1.append(np.max(df[1])/1000)\n",
    "                k1.append(len([x for x in df[1] if x > 16667]))\n",
    "                k1.append(sum([x - 16667 for x in df[1] if x > 16667])/1000)\n",
    "                k.append(k1)\n",
    "    df = pd.DataFrame(k)\n",
    "    df.columns = ['Algorithm', 'Resources', 'Seed', 'SF', 'DF', 'CS']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gather_all_data(algorithms, finalnames, seeds, directory, resources)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the boxplots for a given number of resources and metric\n",
    "def plot_lineplots(data, algorithm, metrics):\n",
    "    color = {metrics[0]:'blue', metrics[1]:'orange', metrics[2]:'green'}\n",
    "    for metric in metrics:\n",
    "        fig,ax = plt.subplots(1,1, figsize=(15,12))\n",
    "        sns.lineplot(x='Resources', y=metric, ci='sd', data=data[algorithm],\n",
    "                     marker='o', color=color[metric])\n",
    "        ax.set(title=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = all_data.loc[all_data['Algorithm'].isin(['FIFO', 'LPT', 'WT', 'CG', 'DCP'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(4, 20)\n",
    "plt.ylim(40, 140)\n",
    "\n",
    "sns.lineplot(x='Resources', y='DF', ci=None, data=plot_data, marker='o', hue='Algorithm',\n",
    "             palette=[colors[0], colors[1], colors[6], colors[9], colors[10]])\n",
    "plt.ylabel('Average Delayed Frames (frames)', fontsize=fontsize)\n",
    "sns.lineplot(x=resources, y=17*[cp_data[0][2]], ci=None, color='gray', label='Critical Path')\n",
    "plt.legend(fontsize=(fontsize-1))\n",
    "plt.xticks(range(4,21,2))\n",
    "\n",
    "plt.savefig(\"../results-3-resources-df.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4\n",
    "**Lineplots with the duration of the frames for FIFO, an algorithm, and the critical path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather data in one dataframe for one number of resources and algorithm\n",
    "def gather_line_data(algorithm, seeds, directory, resource):\n",
    "    data = []\n",
    "    for s in seeds:\n",
    "        filename = file_name_and_path(directory, algorithm, resource, s)\n",
    "        df = pd.read_csv(filename, sep=' ', header=None)\n",
    "        data += [df]\n",
    "    result = pd.concat(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.rcParams['axes.titlesize'] = fontsize\n",
    "plt.rcParams['axes.labelsize'] = fontsize\n",
    "plt.rcParams['xtick.labelsize'] = fontsize\n",
    "plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "plt.xticks(rotation=25)\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(5, 27.500)\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory, '12')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 12 res.', color=colors[9])\n",
    "\n",
    "result = gather_line_data('Coffman', seeds, directory, '20')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='CG w/ 20 res.', color=colors[11])\n",
    "\n",
    "result = gather_line_data('Infinity', seeds, directory_cp, '1000')\n",
    "sns.lineplot(x=list(result[0]), y=list(result[1]/1000), ci=None, label='Critical Path', color='gray')\n",
    "\n",
    "sns.lineplot(x=list(range(1,201)), y=200*[16.667], ci=None, label='16.667 ms', color='red')\n",
    "\n",
    "plt.xlabel('Frame number', fontsize=fontsize)\n",
    "plt.ylabel('Average Frame Duration (ms)', fontsize=fontsize)\n",
    "plt.legend(fontsize=(fontsize-1))\n",
    "plt.xticks(range(0,201,25))\n",
    "           \n",
    "plt.savefig(\"../results-4-frame-duration.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix - input files used to generate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_scenario_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../input_CP_scenario_1.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
